#LyX 1.4.3 created this file. For more info see http://www.lyx.org/
\lyxformat 245
\begin_document
\begin_header
\textclass icml2007
\begin_preamble
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{mlapa}
\end_preamble
\language english
\inputencoding latin1
\fontscheme default
\graphics default
\paperfontsize 10
\spacing single
\papersize default
\use_geometry false
\use_amsmath 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Standard


\backslash
twocolumn[
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Standard


\backslash
icmltitle{Hierarchical Gaussian Process Latent Variable Models}
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard


\backslash
icmlauthor{Neil D.
 Lawrence}{neill@cs.man.ac.uk}
\end_layout

\begin_layout Standard


\backslash
icmladdress{School of Computer Science, University of Manchester, Kilburn
 Building, Oxford Road, Manchester, M13 9PL, U.K.}
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard


\backslash
icmlauthor{Andrew J.
 Moore}{A.Moore@dcs.shef.ac.uk}
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard


\backslash
icmladdress{Dept of Computer Science, University of Sheffield, Regent Court,
 211 Portobello Street, Sheffield, S1 4DP, U.K.}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Standard

\end_layout

\begin_layout Standard

]
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
The Gaussian process latent variable model (GP-LVM) is a powerful approach
 for probabilistic modelling of high dimensional data through dimensional
 reduction.
 In this paper we extend the GP-LVM through hierarchies.
 A hierarchical model (such as a tree) allows us to express conditional
 independencies in the data as well as the manifold structure.
 We first introduce Gaussian process hierarchies through a simple dynamical
 model, we then extend the approach to a more complex hierarchy which is
 applied to the visualisation of human motion data sets.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The Gaussian process latent variable model 
\begin_inset LatexCommand \cite{Lawrence:gplvm03,Lawrence:pnpca05}

\end_inset

 has proven to be a highly effective approach to probabilistic modelling
 of high dimensional data that lies on a non-linear manifold 
\begin_inset LatexCommand \cite{Grochow:styleik04,Urtasun:priors05,Urtasun:3dpeople06,Ferris:wifi07}

\end_inset

.
 The curse of dimensionality is finessed by assuming that the high dimensional
 data is intrinsically low dimensional in nature.
 This reduces the effective number of parameters in the model enabling good
 generalisation from very small data sets using non-linear models (even
 when the dimensionality of the features, 
\begin_inset Formula $d$
\end_inset

, is larger than the number of data points, 
\begin_inset Formula $N$
\end_inset

).
\end_layout

\begin_layout Standard
One alternative to manifold representations when modelling high dimensional
 data is to develop a latent variable model with sparse connectivity to
 explain the data.
 For example tree structured models have been suggested for modelling images
 
\begin_inset LatexCommand \cite{Williams:tree98,Feng:combining02,Awasthi:image07}

\end_inset

, for object recognition 
\begin_inset LatexCommand \cite{Felzenszwalb:efficient00,Ioffe:mixtures01}

\end_inset

 and human pose estimation 
\begin_inset LatexCommand \cite{Ramanan:finding03,Sigal:loose04,Lan:beyond05}

\end_inset

.
 From the probabilistic perspective 
\begin_inset LatexCommand \cite{Pearl:book88}

\end_inset

 the tree structures (and other sparse probabilistic models) offer a convenient
 way to specify conditional independencies in the model.
 In general, it is not clear how such conditional independencies can be
 specified within the context of dimensional reduction.
 In this paper we will show how we can construct our dimensionality reduction
 in a hierarchical way allowing us to concurrently exploit the advantages
 of expressing conditional independencies and low dimensional non-linear
 manifolds.
\end_layout

\begin_layout Subsection
GP-LVMs
\end_layout

\begin_layout Standard
The Gaussian process latent variable model (GP-LVM) is a fully probabilistic,
 non-linear, latent variable model that generalises principal component
 analysis.
 The model was inspired by the observation that a particular probabilistic
 interpretation of PCA is a product of Gaussian process models each with
 a 
\emph on
linear
\emph default
 covariance function.
 Through consideration of non-linear covariance functions a non-linear latent
 variable model can be constructed 
\begin_inset LatexCommand \cite{Lawrence:gplvm03,Lawrence:pnpca05}

\end_inset

.
\end_layout

\begin_layout Standard
An important characteristic of the GP-LVM is the ease and accuracy with
 which probabilistic reconstructions of the data can be made, given a (possibly
 new) point in the latent space.
 This characteristic is exploited in several of the successful applications
 of the GP-LVM: learning style from motion capture data 
\begin_inset LatexCommand \cite{Grochow:styleik04}

\end_inset

 learning a prior model for tracking 
\begin_inset LatexCommand \cite{Urtasun:priors05,Urtasun:3dpeople06}

\end_inset

 and robot simultaneous localisation and mapping 
\begin_inset LatexCommand \cite{Ferris:wifi07}

\end_inset

.
 All make use of smooth mappings from the latent space to the data space.
\end_layout

\begin_layout Standard
The probabilistic approach to non-linear dimensionality reduction 
\begin_inset LatexCommand \cite{MacKay:wondsa95,Bishop:gtm_ncomp98}

\end_inset

 is to formulate a latent variable model, where the latent dimension, 
\begin_inset Formula $q$
\end_inset

, is lower than the data dimension, 
\begin_inset Formula $d$
\end_inset

.
 The latent space is then governed by a prior distribution 
\begin_inset Formula $p\left(\mathbf{X}\right)$
\end_inset

.
 The latent variable is related to the observation space through a probabilistic
 mapping,
\begin_inset Formula \[
y_{ni}=f_{i}\left(\mathbf{x}_{n};\mathbf{W}\right)+\epsilon_{n},\]

\end_inset

 where 
\begin_inset Formula $y_{ni}$
\end_inset

 is the 
\begin_inset Formula $i$
\end_inset

th feature of the 
\begin_inset Formula $n$
\end_inset

th data point and 
\begin_inset Formula $\epsilon_{n}$
\end_inset

 is a noise term that is typically taken to be Gaussian
\begin_inset Foot
status collapsed

\begin_layout Standard
We denote a Gaussian distribution over 
\begin_inset Formula $\mathbf{z}$
\end_inset

 with mean 
\begin_inset Formula $\boldsymbol{\mu}$
\end_inset

 and covariance 
\begin_inset Formula $\boldsymbol{\Sigma}$
\end_inset

 by 
\begin_inset Formula $N\left(\mathbf{z}|\boldsymbol{\mu,\boldsymbol{\Sigma}}\right)$
\end_inset

.
\end_layout

\end_inset

, 
\begin_inset Formula $p\left(\epsilon_{n}\right)=N\left(\epsilon_{n}|0,\beta^{-1}\right).$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}$
\end_inset

 is a matrix of mapping parameters.
 If the prior is taken to be independent across data points the marginal
 likelihood of the data can be written as 
\begin_inset Formula \[
p\left(\mathbf{Y}|\mathbf{W}\right)=\int\prod_{n=1}^{N}p\left(\mathbf{y}_{n}|\mathbf{x}_{n},\mathbf{W}\right)p\left(\mathbf{x}_{n}\right)\,\textrm{d}\,\mathbf{X},\]

\end_inset

 where 
\begin_inset Formula $p\left(\mathbf{y}_{n}|\mathbf{x}_{n}\right)=\prod_{i=1}^{d}N\left(y_{in}|f_{in}\left(\mathbf{x}_{n}\right),\beta^{-1}\right).$
\end_inset

 If the mapping is chosen to be linear, 
\begin_inset Formula $f_{i}\left(\mathbf{x}_{n}\right)=\mathbf{w}_{i}^{\textrm{T}}\mathbf{x}_{n}$
\end_inset

, and the prior over the latent variables is taken to be Gaussian, then
 the maximum likelihood solution of the model spans the principal subspace
 of the data 
\begin_inset LatexCommand \cite{Tipping:probpca99}

\end_inset

.
 However, if the mapping is non-linear it is unclear, in general, how to
 propagate the prior distribution's uncertainty through the non-linearity.
 
\end_layout

\begin_layout Standard
The alternative approach taken by the GP-LVM is to place the prior distribution
 over the mappings rather than the latent variables.
 The mappings may then be marginalised and the marginal likelihood optimised
 with respect to the latent variables, 
\begin_inset Formula \begin{equation}
p\left(\mathbf{Y}|\mathbf{X}\right)=\int\prod_{i=1}^{d}\prod_{n=1}^{N}p\left(y_{in}|f_{in}\right)p\left(\mathbf{f}|\mathbf{X}\right)\mbox{d}\,\mathbf{f}.\label{eq:gplvmMarginalisation}\end{equation}

\end_inset

It turns out that if the prior is taken to be a Gaussian process that is
 independent across data dimensions and has a 
\emph on
linear
\emph default
 covariance function (thus restricting the mappings to linearity) the maximum
 likelihood solution with respect to the embeddings is given by principal
 component analysis.
 However, if the covariance function is one which allows non-linear functions
 (
\emph on
e.g.

\emph default
 the RBF kernel) then the model provides a probabilistic non-linear latent
 variable model.
 
\end_layout

\begin_layout Standard
There are several advantages to marginalising the mapping rather than the
 latent variable.
 In particular, a non-linear latent variable model that does not require
 approximations is recovered.
 Additionally, we now have a probabilistic model of the data that is expressed
 in the form 
\begin_inset Formula $p\left(\mathbf{Y}|\mathbf{X}\right)$
\end_inset

 rather than the more usual form 
\begin_inset Formula $p\left(\mathbf{Y}|\mathbf{W}\right)$
\end_inset

.
 Our model is non-parametric, the size of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is 
\begin_inset Formula $N\times q$
\end_inset

 and each row of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, given by 
\begin_inset Formula $\mathbf{x}_{n}$
\end_inset

, is associated with an data observation, 
\begin_inset Formula $\mathbf{y}_{n}$
\end_inset

.
 This makes it much easier to augment the model with additional constraints
 or prior information about the data.
 Interesting examples include adding dynamical priors in the latent space
 
\begin_inset LatexCommand \cite{Wang:gpdm05,Urtasun:3dpeople06}

\end_inset

 or constraining points in the latent space according to intuitively reasonable
 visualisation criteria 
\begin_inset LatexCommand \cite{Lawrence:backconstraints06}

\end_inset

.
 In this paper we further exploit this characteristic, proposing the hierarchica
l Gaussian process latent variable models.
 In the next section we will illustrate the nature of a simple (one layered)
 hierarchical model by considering a novel approach to incorporating 
\emph on
dynamics
\emph default
 into the GP-LVM, then in Section\InsetSpace ~

\begin_inset LatexCommand \ref{sec:complexHierarchies}

\end_inset

 we consider more complex hierarchies, focussing on models of human body
 motion.
 
\end_layout

\begin_layout Section
Dynamics via a Simple Hierarchy
\end_layout

\begin_layout Standard
In a standard latent variable model setting, a dynamical system is modelled
 by constructing a dynamical prior distribution that, for tractability,
 typically takes the form of a Markov chain, 
\begin_inset Formula $p\left(\mathbf{X}\right)=p\left(\mathbf{x}_{1}\right)\prod_{t=2}^{T}p\left(\mathbf{x}_{t}|\mathbf{x}_{t-1}\right)$
\end_inset

.
 The latent variable, 
\begin_inset Formula $\mathbf{X}$
\end_inset

, is marginalised as before, inducing correlations between 
\emph on
neighbouring
\emph default
 time points.
 In the GP-LVM we marginalise with respect to the mapping, once this marginalisa
tion is performed, integrating out the latent space and any associated dynamical
 prior analytically intractable.
 However, we may instead choose combine a dynamical prior with the GP-LVM
 likelihood and seek a 
\emph on
maximum a posteriori
\emph default
 (MAP) solution.
 
\end_layout

\begin_layout Subsection
Gaussian Process Dynamics
\begin_inset LatexCommand \label{sub:gaussianProcessDynamics}

\end_inset


\end_layout

\begin_layout Standard
Seeking a MAP solution is the approach taken by 
\begin_inset LatexCommand \cite{Wang:gpdm05}

\end_inset

 who make use of an autoregressive Gaussian process prior to augment the
 GP-LVM with dynamics.
 The utility of the approach is nicely demonstrated in the context of tracking
 by 
\begin_inset LatexCommand \cite{Urtasun:3dpeople06}

\end_inset

 who show that through dynamics the track is sustained even when the subject
 is fully occluded for several frames.
\end_layout

\begin_layout Standard
The autoregressive approach works by predicting the next temporal location
 in the latent space given the previous, 
\emph on
i.e.
 
\emph default
it models 
\begin_inset Formula $p\left(\mathbf{x}_{t}|\mathbf{x}_{t-1}\right)$
\end_inset

.
 However, since the prediction is given by a Gaussian process it is a unimodal
 prediction over 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 given 
\begin_inset Formula $\mathbf{x}_{t-1}$
\end_inset

.
 This can present problems: consider, for example, the case of a subject
 walking for several paces before breaking into a run.
 We expect the walking steps to be broadly periodic, each point from the
 cycle projecting into a similar point in latent space.
 However, at the point the subject begins to break into a run, there is
 a bifurcation in the dynamics.
 Such a bifurcation can not be captured correctly by unimodal autoregressive
 dynamics.
 Additionally, the autoregressive approach assumes that samples are taken
 at uniform intervals (perhaps with occasional drop outs) which may not
 always be the case (as we shall see in Section\InsetSpace ~

\begin_inset LatexCommand \ref{sec:complexHierarchies}

\end_inset

).
 
\end_layout

\begin_layout Subsection
A Simple Hierarchical Model
\begin_inset LatexCommand \label{sub:regressiveDynamics}

\end_inset


\end_layout

\begin_layout Standard
As the first illustration of a hierarchical GP-LVM we consider an alternative
 implementation of dynamics.
 Just as 
\begin_inset LatexCommand \cite{Wang:gpdm05}

\end_inset

 we implement the dynamics through a Gaussian process prior and seek a MAP
 solution.
 However, in contrast to their approach, our model is not autoregressive.
 We simply place a Gaussian process prior over the latent space, the inputs
 for which are given by the time points, 
\begin_inset Formula $\mathbf{t}$
\end_inset

.
 This approach alleviates the requirement for uniform intervals between
 time samples and, because the prior over the latent space is no longer
 a function of the location in latent space, allows the path in latent space
 to bifurcate at points where the subject, for example, breaks into a run.
\end_layout

\begin_layout Standard
Given a set of 
\begin_inset Formula $d$
\end_inset

-dimensional observations from a motion capture sequence, 
\begin_inset Formula $\mathbf{Y}=\left[\mathbf{y}_{1,:},\dots,\,\mathbf{y}_{T,:}\right]^{\mbox{T}}\in\Re^{T\times d}$
\end_inset

, we seek to model them by a Gaussian process latent variable model, 
\begin_inset Formula \begin{equation}
p\left(\mathbf{Y}|\mathbf{X}\right)=\prod_{j=1}^{d}N\left(\mathbf{y}_{:,j}|\mathbf{0},\mathbf{K}_{x}\right),\label{eq:gplvmLikelihood}\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{y}_{:,j}$
\end_inset

 is a column of the design matrix 
\begin_inset Formula $\mathbf{Y}$
\end_inset

, each element being from a different point in the time sequence, and 
\begin_inset Formula $\mathbf{K}_{x}$
\end_inset

 is a covariance matrix (or kernel) which depends on the 
\begin_inset Formula $q$
\end_inset

-dimensional latent variables, 
\begin_inset Formula $\mathbf{X}=\left[\mathbf{x}_{1},\dots,\,\mathbf{x}_{T}\right]^{\mbox{T}}\in\Re^{T\times q}$
\end_inset

, each element being given by, for example, 
\begin_inset Formula \[
k_{x}\left(\mathbf{x}_{i},\mathbf{x}_{j}\right)=\sigma_{\mbox{rbf}}^{2}\exp\left(-\frac{\left\Vert \mathbf{x}_{i}-\mathbf{x}_{j}\right\Vert ^{2}}{2l_{x}^{2}}\right)+\sigma_{\mbox{white}}^{2}\delta_{ij},\]

\end_inset

which is an radial basis function (RBF) covariance matrix with a noise term.
 The parameters of this covariance are the variances of the different terms
 
\begin_inset Formula $\sigma_{\mbox{rbf}}^{\mbox{2}},\,\sigma_{\mbox{white}}^{2}$
\end_inset

 and the length scale of the RBF term, 
\begin_inset Formula $l_{x}$
\end_inset

.
 In (
\begin_inset LatexCommand \ref{eq:gplvmLikelihood}

\end_inset

) we have dropped the dependence on these parameters to avoid cluttering
 notation.
\end_layout

\begin_layout Standard
We construct a simple hierarchy by placing a prior over the elements of
 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 We wish this prior to be temporally smooth, ensuring two points from 
\begin_inset Formula $\mathbf{X}$
\end_inset

 that are temporally close, 
\emph on
e.g.

\emph default
 
\begin_inset Formula $\mathbf{x}_{i,:}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{j,:}$
\end_inset

 are also nearby in space.
 A suitable prior is given by a Gaussian process in which the input to the
 Gaussian process is time,
\begin_inset Formula \begin{equation}
p\left(\mathbf{X}|\mathbf{t}\right)=\prod_{i=1}^{q}N\left(\mathbf{x}_{:,i}|\mathbf{0},\mathbf{K}_{t}\right),\label{eq:gpTemporalPrior}\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{t}\in\Re^{T\times1}$
\end_inset

 is the vector of times at which we observed the sequence, 
\begin_inset Formula $\mathbf{x}_{:,j}$
\end_inset

 is the 
\begin_inset Formula $j$
\end_inset

th column of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{K}_{t}$
\end_inset

 is a covariance matrix of the form
\begin_inset Formula \[
k_{t}\left(t_{i},t_{j}\right)=\varsigma_{\mbox{rbf}}^{2}\exp\left(-\frac{\left(t_{i}-t_{j}\right)^{2}}{2l_{t}^{2}}\right)+\varsigma_{\mbox{white}}^{\mbox{2}}.\]

\end_inset

For a two dimensional latent space typical sample paths for this covariance
 function are shown in Figure\InsetSpace ~

\begin_inset LatexCommand \ref{fig:Typical-sample-paths}

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../diagrams/demTemporalSamplePaths.eps
	width 80col%

\end_inset


\end_layout

\begin_layout Caption
Typical sample paths for the RBF covariance function as temporal prior over
 the latent space.
\begin_inset LatexCommand \label{fig:Typical-sample-paths}

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
The temporal prior in (
\begin_inset LatexCommand \ref{eq:gpTemporalPrior}

\end_inset

) can be combined with the GP-LVM likelihood in (
\begin_inset LatexCommand \ref{eq:gplvmLikelihood}

\end_inset

) to form a new model,
\begin_inset Formula \[
p\left(\mathbf{Y}|\mathbf{t}\right)=\int p\left(\mathbf{Y}|\mathbf{X}\right)p\left(\mathbf{X}|\mbox{\textbf{t}}\right)\mbox{d}\mathbf{X},\]

\end_inset

unfortunately such a marginalisation is intractable.
 Instead, we seek to make progress by seeking a 
\emph on
maximum a posteriori
\emph default
 (MAP) solution, maximising
\begin_inset Formula \[
\log p\left(\mathbf{X}|\mathbf{Y},\mathbf{t}\right)=\log p\left(\mathbf{Y}|\mathbf{X}\right)+\log p\left(\mathbf{X}|\mathbf{t}\right)+\mbox{const.}\]

\end_inset

with respect to 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 The first term in this equation is the standard objective function for
 the GP-LVM, the second term has the form
\begin_inset Formula \[
\log p\left(\mathbf{X}|\mathbf{t}\right)=-\frac{1}{2}\prod_{j=1}^{q}\mathbf{x}_{:,j}^{\mbox{T}}\mathbf{K}_{t}^{-1}\mathbf{x}_{:,j}+\mbox{const.},\]

\end_inset

where 
\begin_inset Formula $\mathbf{x}_{:,j}$
\end_inset

 is the 
\begin_inset Formula $j$
\end_inset

th column of 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 The gradient of this additional term may also be found,
\begin_inset Formula \[
\frac{\mbox{d}\log p\left(\mathbf{X}|\mathbf{t}\right)}{\mbox{d}\mathbf{X}}=\mathbf{K}_{t}^{-1}\mathbf{X}\]

\end_inset

and combined with the gradient of 
\begin_inset Formula $\log p\left(\mathbf{Y}|\mathbf{X}\right)$
\end_inset

 to find the MAP solution.
 This can easily be found using gradient based methods.
\end_layout

\begin_layout Section
More Complex Hierarchies
\begin_inset LatexCommand \label{sec:complexHierarchies}

\end_inset


\end_layout

\begin_layout Standard
We now turn to a slightly more complex hierarchy than the dynamical model
 described in the previous section.
 Consider a motion capture example with multiple subjects interacting.
 Given the form of the interaction it should be possible to model each subject
 independently.
 This form of conditional independence is well captured by a hierarchical
 model such as that shown in Figure\InsetSpace ~

\begin_inset LatexCommand \ref{fig:twoSubjects}

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../diagrams/twoSubjects.eps
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Caption
A simple hierarchy for capturing interaction between two subjects where
 
\begin_inset Formula $\mathbf{Y}_{1}$
\end_inset

 is the data associated with subject 1, 
\begin_inset Formula $\mathbf{Y}_{2}$
\end_inset

 is that of subject 2.
 Each of these variable sets is then controlled by latent variables, 
\begin_inset Formula $\mathbf{X}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{X}_{2}$
\end_inset

.
 These latent variables are in turn controlled by 
\begin_inset Formula $\mathbf{X}_{3}$
\end_inset

.
 
\begin_inset LatexCommand \label{fig:twoSubjects}

\end_inset


\end_layout

\end_inset

The joint probability distribution represented by this graph is given by
\begin_inset Formula \begin{eqnarray*}
p\left(\mathbf{Y}_{1},\mathbf{Y}_{2}\right) & = & \int p\left(\mathbf{Y}_{1}|\mathbf{X}_{1}\right)\dots\\
 &  & \times\int p\left(\mathbf{Y}_{2}|\mathbf{X}_{2}\right)\dots\\
 &  & \times\int p\left(\mathbf{X}_{1},\mathbf{X}_{2}|\mathbf{X}_{3}\right)\mbox{d}\mathbf{X}_{3}\mbox{d}\mathbf{X}_{2}\mbox{d}\mathbf{X}_{1}\\
 &  & ,\end{eqnarray*}

\end_inset

where each conditional distribution is given by a Gaussian process.
 However, once again, the required marginalisations are not tractable.
 We therefore turn to MAP solutions for finding the values of the latent
 variables.
 For this model, this means maximisation of
\begin_inset Formula \begin{eqnarray*}
\log p\left(\mathbf{X}_{1},\mathbf{X}_{2}\mathbf{X}_{3}|\mathbf{Y}_{1},\mathbf{Y}_{2}\right) & = & \log p\left(\mathbf{Y}_{1}|\mathbf{X}_{1}\right)\\
 &  & +\log p\left(\mathbf{Y}_{2}|\mathbf{X}_{2}\right)\\
 &  & +\log p\left(\mathbf{X}_{1},\mathbf{X}_{2}|\mathbf{X}_{3}\right),\end{eqnarray*}

\end_inset

which is the sum of three Gaussian process log likelihoods.
 The first two terms are associated with the two subjects.
 The third term provides co-ordination between the subjects.
 
\end_layout

\begin_layout Subsection
Two Interacting Subjects
\end_layout

\begin_layout Standard
To demonstrate this hierarchical model we considered a motion capture data
 set consisting of two interacting subjects.
 The data, which was taken from the CMU MOCAP data base
\begin_inset Foot
status open

\begin_layout Standard
\begin_inset LatexCommand \htmlurl{http://mocap.cs.cmu.edu}

\end_inset

.
\end_layout

\end_inset

, consists of two subjects
\begin_inset Foot
status collapsed

\begin_layout Standard
The subjects used are numbered 20 and 21 in the data base.
 The motion is number 11.
 
\end_layout

\end_inset

 that approach each other and `high five'.
 
\end_layout

\begin_layout Standard
The algorithm for optimisation of the latent variables proceeded as follows:
\end_layout

\begin_layout Enumerate
Initialise each leaf node's latent variable set (
\begin_inset Formula $\mathbf{X}_{1},\mathbf{X}_{2}$
\end_inset

) through principal component analysis of the corresponding data set (
\begin_inset Formula $\mathbf{Y}_{1}$
\end_inset

,
\begin_inset Formula $\mathbf{Y}_{2}$
\end_inset

).
\end_layout

\begin_layout Enumerate
Initialise the root node's latent variable set (
\begin_inset Formula $\mathbf{X}_{3}$
\end_inset

) through principal component analysis of the concatenated latent variables
 of its dependents 
\begin_inset Formula $\left[\mathbf{X}_{1}\,\,\mathbf{X}_{2}\right]$
\end_inset

.
\end_layout

\begin_layout Enumerate
Optimise jointly the parameters of the kernel matrices for each Gaussian
 process model and the latent variable positions 
\begin_inset Formula $\left(\mathbf{X}_{1},\mathbf{X}_{2},\mathbf{X}_{3}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
The original data is sampled at 120 frames per second.
 We extracted frames 50 to 113, sub-sampling to 30 frames per second, frames
 114 to 155 at the full sample rate and frames 156 to 232 sub-sampling at
 30 frames per second.
 This gives a data set with a variable sample rate.
 In the context of this data the variable sample rate is important: the
 section where we used the higher sample rate contains the slapping of the
 two subjects hands.
 This motion is rapid and cannot be accurately reconstructed with a sample
 rate of 30 frames per second.
 This variable sample rate presents problems for the autoregressive dynamics
 we reviewed in Section\InsetSpace ~

\begin_inset LatexCommand \ref{sub:gaussianProcessDynamics}

\end_inset

.
 However, for the regressive dynamics we introduced in Section\InsetSpace ~

\begin_inset LatexCommand \ref{sub:regressiveDynamics}

\end_inset

 the variable sample rate can simply be reflected in the vector 
\begin_inset Formula $\mathbf{t}$
\end_inset

.
 We therefore made use of these dynamics by adding a further layer to the
 hierarchy,
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
p\left(\mathbf{Y}_{1},\mathbf{Y}_{2}|\mathbf{t}\right) & = & \int p\left(\mathbf{Y}_{1}|\mathbf{X}_{1}\right)\dots\\
 &  & \times\int p\left(\mathbf{Y}_{2}|\mathbf{X}_{2}\right)\int p\left(\mathbf{X}_{1},\mathbf{X}_{2}|\mathbf{X}_{3}\right)\dots\\
 &  & \times p\left(\mathbf{X}_{3}|\mathbf{t}\right)\mbox{d}\mathbf{X}_{3}\mbox{d}\mathbf{X}_{2}\mbox{d}\mathbf{X}_{1}.\end{eqnarray*}

\end_inset

However, we do not optimise the parameters of the dynamics: we wish the
 latent space to be constrained by the dynamics.
 Finally, we would like the effect of the dynamics to be present as we descend
 the the hierarchy.
 To this end, we constrained the noise parameter, 
\begin_inset Formula $\sigma_{\mbox{white}}^{2}$
\end_inset

 of the Gaussian process associated with 
\begin_inset Formula $p\left(\mathbf{X}_{1},\mathbf{X}_{2}|\mathbf{X}_{3}\right)$
\end_inset

 to 
\begin_inset Formula $1\times10^{-6}$
\end_inset

.
 If we allow this variance to be free, the effect of the dynamics could
 become diluted as we drop down the hierarchy.
 By constraining this variance we force the temporal correlations present
 in the data to be respected.
\end_layout

\begin_layout Standard
In Figure\InsetSpace ~

\begin_inset LatexCommand \ref{fig:highFive}

\end_inset

 we show the results of mapping these motions into this hierarchical model.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide true
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../diagrams/demHighFive_talk.eps
	width 70text%

\end_inset


\end_layout

\begin_layout Caption
High Five example.
 Two subjects are modelled as they walk towards each other and `high five'.
 The plot shows the simple hierarchy used to model the data.
 There is a regressive dynamical prior of the type described in Section\InsetSpace ~

\begin_inset LatexCommand \ref{eq:gpTemporalPrior}

\end_inset

 placed over the latent space of the root node.
 The root node then controls the two individual subjects.
 To illustrate the model we have taken points at time (i.e we input these
 values of 
\begin_inset Formula $t$
\end_inset

 into the dynamical model) frames A: 85, B: 114, C:127, D: 141, E: 155,
 F: 170, G: 190 and H: 215.
 These points mapped down through the hierarchy and into the data space.
 In each of the plots of the two subjects, Subject 1 is on the right and
 Subject 2 is on the left.
 
\begin_inset LatexCommand \label{fig:highFive}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Subject Decomposition
\end_layout

\begin_layout Standard
As well as decomposing the interactions between two subjects into a hierarchy,
 we can also consider decomposition of a single subject into parts.
 As we discussed in the introduction, there have been several different
 approaches to modelling motion capture data through tree based models,
 but these models typically assume that the nodes of the tree are observed
 and that the tree rigidly reflects the skeletal structure.
 Some effort has been made to model additional correlations in motion data
 by augmenting the tree with an additional, common, latent variable 
\begin_inset LatexCommand \cite{Lan:beyond05}

\end_inset

.
 However, our hierarchical model is closer in structure to the tree models
 of 
\begin_inset LatexCommand \cite{Williams:tree98}

\end_inset

 where the tree structure refers to a 
\emph on
hierarchy of latent variables
\emph default
, rather than a hierarchy of the observed variables.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../diagrams/stickHierarchy.eps
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Caption
Decomposition of skeleton for hierarchical modelling.
 By separating the component parts of the skeleton in this manner we can
 model the space of natural motions for each component part and express
 them independently or jointly.
\begin_inset LatexCommand \label{fig:skeletonDecomposition}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We considered a data set composed of a walking motion and a running motion,
 again taken from the CMU MOCAP data base.
 The run was taken from motion 25 of subject 35 and the walk was taken from
 motion 01 of subject 35.
 The data was sub-sampled to 30 frames per second and one cycle of each
 motion was used.
 The x and y location of each motion's `root position' was set to zero so
 that the subject was running/walking `in place'.
 We modelled the subject using the decomposition shown in Figure\InsetSpace ~

\begin_inset LatexCommand \ref{fig:skeletonDecomposition}

\end_inset

, but to reflect the fact that two different motions were present in the
 data we constructed a hierarchy with 
\emph on
two roots
\emph default
.
 One root was associated with the run and a second root was associated with
 the walk.
 The prior induced by the run root was applied only to the run data points
 in the next layer of the hierarchy (abdomen, legs, upper body).
 Similarly, the prior induced by the walk root was applied only to data
 points from the walk data.
 The upper body, legs and all the leaf nodes were applied to the entire
 data set.
 This construction enables us to express the two motion sequences separately
 whilst retaining the information required to jointly model the component
 parts of the skeleton.
 The aim is for nodes in the lower levels of the hierarchy to span the range
 of motions, whilst the upper layer specifies the particular motion type.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../diagrams/demWalkRun_talk.eps
	lyxscale 70
	width 70text%

\end_inset


\end_layout

\begin_layout Caption
Combined model of a run and a walk.
 The skeleton is decomposed as shown in Figure\InsetSpace ~

\begin_inset LatexCommand \ref{fig:skeletonDecomposition}

\end_inset

.
 In the plots, crosses are latent positions associated with the run and
 circles are associated with the walk.
 We have mapped three points from each motion through the hierarchy.
 Periodic dynamics was used in the latent spaces.
\begin_inset LatexCommand \label{fig:demRunWalk1}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As the motion is broadly periodic, we made use of a periodic kernel 
\begin_inset LatexCommand \cite{MacKay:gpintroduction98}

\end_inset

 for the regressive dynamics in each latent space (see pg.
 92 in 
\begin_inset LatexCommand \cite{Rasmussen:book06}

\end_inset

 for details).
 The resulting visualisation is shown in Figure\InsetSpace ~

\begin_inset LatexCommand \ref{fig:demRunWalk1}

\end_inset

.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
We have presented a hierarchical version of the Gaussian process latent
 variable model.
 The Gaussian process latent variable model involves a paradigm shift in
 probabilistic latent variable models where, rather than marginalising the
 latent variables and optimising the mappings, we marginalise the mappings
 and optimise the latent variables.
 This makes far easier to construct hierarchies of these models.
 The philosophy of optimising versus marginalising is carried through to
 the hierarchical GP-LVM: we maximise with respect to all the latent variables
 in the different levels of the hierarchy.
\end_layout

\begin_layout Subsection
Overfitting
\end_layout

\begin_layout Standard
Modelling with the GP-LVM is characterised by the use of very large numbers
 of `parameters' in the form of the latent points.
 In the standard case, the number of parameters increases linearly as a
 fraction, 
\begin_inset Formula $\frac{q}{d}$
\end_inset

, of the number of data.
 As long as 
\begin_inset Formula $q<d$
\end_inset

 (how much less depends on the data set) problems of overfitting do not
 normally occur.
 However, we are now adding additional latent variables, do we not now run
 the risk of overfitting the data if the hierarchy becomes too deep? The
 first point to note is that the upper levels of the hierarchy only serve
 to regularise the leaf nodes: so if the leaf nodes independently do not
 overfit, neither will the entire model.
 In other words, we must ensure that the leaf nodes each have 
\begin_inset Formula $q_{i}<d_{i}$
\end_inset

 where 
\begin_inset Formula $q_{i}$
\end_inset

 is the number of columns of 
\begin_inset Formula $\mathbf{X}_{i}$
\end_inset

 and 
\begin_inset Formula $d_{i}$
\end_inset

 is the dimensionality of 
\begin_inset Formula $\mathbf{Y}_{i}$
\end_inset

.
 However, by modifying the locations of latent variables in nodes higher
 up the hierarchy we are changing the nature of the 
\emph on
regularisation
\emph default
 of the leaf nodes.
 If unconstrained the model could simply act in such a way as to remove
 the regularisation.
 In our implementation we attempted to counter this potential problem in
 two ways.
 Firstly, we provided a fixed dynamical prior at the top level.
 The parameters of this prior were not optimised, so the top level node
 is always `regularised
\begin_inset Foot
status collapsed

\begin_layout Standard
The same goal could also be achieved through 
\emph on
back constraints
\emph default
 
\begin_inset LatexCommand \cite{Lawrence:backconstraints06}

\end_inset

, but we did not explore that approach here.
\end_layout

\end_inset

'.
 However, there is the possibility that this fixed regularisation could
 be `diluted' by noise as we descend the hierarchy.
 To prevent this happening we constrained the noise variance of each Gaussian
 process that was not in a leaf node to 
\begin_inset Formula $1\times10^{-6}$
\end_inset

, 
\emph on
i.e.
 
\emph default
close to zero but high enough to prevent numerical instabilities in kernel
 matrix inverses.
 This strategy proved effective in all our experiments.
\end_layout

\begin_layout Subsection
Other Hierarchical Models
\end_layout

\begin_layout Standard
Given apparent similarities between the model names, it is natural to ask
 what is the relationship between the hierarchical GP-LVM and the hierarchical
 probabilistic PCA of 
\begin_inset LatexCommand \cite{Bishop:hierarchy98}

\end_inset

? The two models are philosophically distinct.
 In hierarchical PCA (and the related hierarchical GTM model of 
\begin_inset LatexCommand \cite{Tino:hierarchical02}

\end_inset

) every node in the hierarchy is associated with a probabilistic model in
 
\emph on
data space
\emph default
.
 The hierarchy is not a hierarchy of latent variables, it is, instead, a
 hierarchical clustering of mixture components in a discrete mixture of
 probabilistic PCA models (or GTM models).
 A similar approach could be taken with the GP-LVM, but it is not the approach
 we have described here.
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
We see the hierarchical GP-LVM as an important tool in several application
 areas.
 However, there are two application areas in which we believe the algorithm
 has particular promise.
 Firstly, the GP-LVM has already been proposed as a prior model for tracking.
 A key problem with constructing such prior models is that it is difficult
 to cover the space of all natural human motions.
 However, using the hierarchical model we expect, inspired by language modelling
, to be able to perform a variant of `back off'.
 Depending on motion, different models could be swapped in at the top level
 of the hierarchy, however some actions will still not be well modelled.
 In this case we suggest `backing off', which in this context would translate
 into dropping down the hierarchy and applying the models in the next layer
 of the hierarchy 
\emph on
independently
\emph default
 to the data.
 Another application area where we see great promise for the model is animation.
 Through the hierarchical GP-LVM model different portions of the a character
 can be animated separately or jointly as circumstances demand.
 Animator time is becoming a dominating cost in both the games and film
 entertainment industries where computer special effect techniques are used,
 through combination of the hierarchical GP-LVM with appropriate inverse
 kinematic techniques 
\begin_inset LatexCommand \cite{Grochow:styleik04}

\end_inset

 we could seek to ameliorate these costs.
\end_layout

\begin_layout Subsection*
Acknowledgements
\end_layout

\begin_layout Standard
This work was funded by the EU FP6 PASCAL Network of Excellence under a
 pump priming grant.
 The motion capture data used in this project was obtained from 
\family typewriter
mocap.cs.cmu.edu
\family default
.
 The database was created with funding from NSF EIA-0196217.
 We thank Raquel Urtasun for helpful discussions.
\end_layout

\begin_layout Section
\start_of_appendix
Recreating the Experiments
\end_layout

\begin_layout Standard
The source code for re-running all the experiments detailed here is available
 from 
\begin_inset LatexCommand \htmlurl{http://www.cs.man.ac.uk/~neill/hgplvm/}

\end_inset

, release 0.1.
 
\end_layout

\begin_layout Standard
\begin_inset LatexCommand \bibtex[mlapa]{lawrence,other,zbooks}

\end_inset


\end_layout

\end_body
\end_document
